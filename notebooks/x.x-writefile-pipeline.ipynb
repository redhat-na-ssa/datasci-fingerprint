{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fff30b-4014-4382-ae76-4dd76577d33b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# a Python function that creates subfolders within a given folder\n",
    "#\n",
    "\n",
    "import os\n",
    "\n",
    "def create_subfolders(parent_folder, subfolder_names):\n",
    "    for subfolder in subfolder_names:\n",
    "        subfolder_path = os.path.join(parent_folder, subfolder)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "            \n",
    "parent_folder = '../../scratch'\n",
    "subfolder_names = ['models', 'real', 'train','tune', 'src', '.raw']\n",
    "\n",
    "create_subfolders(parent_folder, subfolder_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e0ba84-fe95-41f9-bc7b-da79e96b97bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/scratch_env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/scratch_env.py\n",
    "\n",
    "#\n",
    "# set environment variables using the os module\n",
    "#\n",
    "\n",
    "import os\n",
    "\n",
    "def set_environment_variable(variable_name, variable_value):\n",
    "    os.environ[variable_name] = variable_value\n",
    "    \n",
    "variable_name = 'SCRATCH'\n",
    "variable_value = '../../scratch'\n",
    "\n",
    "set_environment_variable(variable_name, variable_value)\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', './scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbee4303-6026-4d07-b3bb-a92bad7bdea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/__init__.py\n",
    "\n",
    "#\n",
    "# create an __init__.py to indicate that a directory is a python package.\n",
    "#\n",
    "\n",
    "import scratch_env\n",
    "import install_requirements\n",
    "import git_clone\n",
    "import extract_tar_files\n",
    "import create_image_dataset\n",
    "import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb23fea-56f5-43a7-8aa3-bad6a0e62833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/install_requirements.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/install_requirements.py\n",
    "\n",
    "#\n",
    "# a Python function to install requirements.\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def install_requirements(file_path):\n",
    "    subprocess.run([\"pip\", \"install\", \"--upgrade\", \"pip\"], check=True)\n",
    "    with open(file_path) as f:\n",
    "        packages = f.read().splitlines()\n",
    "\n",
    "    for package in packages:\n",
    "        subprocess.call([f\"pip install {package} -q\"], shell=True)\n",
    "        \n",
    "install_requirements(file_path=\"../../requirements.txt\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e255c8-6f68-4754-80d6-c4fa2eb6a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/git_clone.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/git_clone.py\n",
    "\n",
    "#\n",
    "# use the subprocess module in Python to run shell commands for a git clone \n",
    "#\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def git_clone(repository_url, target_directory):\n",
    "    subprocess.run([\"git\", \"clone\", repository_url, target_directory], check=False)\n",
    "    \n",
    "repository_url = \"https://github.com/redhat-na-ssa/demo-datasci-fingerprint-data.git\"\n",
    "target_directory = \"../../scratch/.raw/\"\n",
    "\n",
    "git_clone(repository_url, target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242e8a7a-8325-46d8-a1b1-d52c0f571ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/extract_tar_files.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/extract_tar_files.py\n",
    "\n",
    "#\n",
    "# a function that extracts the contents of multiple tar archives\n",
    "#\n",
    "\n",
    "import tarfile\n",
    "\n",
    "def extract_tar_files(tar_files, extract_path):\n",
    "    for tar_file in tar_files:\n",
    "        with tarfile.open(tar_file) as tar:\n",
    "            tar.extractall(extract_path)\n",
    "            \n",
    "tar_files = [\"../../scratch/.raw/left.tar.xz\", \"../../scratch/.raw/right.tar.xz\"]\n",
    "extract_path = \"../../scratch/train/\"\n",
    "\n",
    "extract_tar_files(tar_files, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce93f598-da98-4e6a-b106-6d8ab5c16afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/create_image_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/create_image_dataset.py\n",
    "\n",
    "#\n",
    "# a function that creates a dataset\n",
    "#\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# You can then use this function by passing in the desired data and batch size\n",
    "# usage example:  dataset = create_image_dataset(scratch_path + '/train' 32, 'training')\n",
    "def create_image_dataset(data_dir, batch_size, subset):\n",
    "    \"\"\"\n",
    "    Creates a dataset of image data from a directory.\n",
    "    :param data_dir: The directory containing the image data.\n",
    "    :param batch_size: The batch size to use when training.\n",
    "    :param subset: The data subset to be created training, validation, test, etc.\n",
    "    :return: A dataset of image data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the directory\n",
    "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        labels='inferred',\n",
    "        label_mode = \"categorical\", \n",
    "        class_names=['left','right'],\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=batch_size,\n",
    "        image_size=(96, 96),\n",
    "        seed=42,\n",
    "        validation_split=0.3,\n",
    "        subset=subset,\n",
    "        interpolation='nearest'\n",
    "    )\n",
    "    \n",
    "    data_aug = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomRotation(\n",
    "            0.2,\n",
    "            fill_mode='constant',\n",
    "            interpolation='nearest',\n",
    "            seed=None,\n",
    "            fill_value=0.0\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Use the AUTOTUNE option for optimal data loading performance\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# create the train and validation and optimize the datasets\n",
    "train_ds = create_image_dataset(data_dir=\"../../scratch/train/\", batch_size=32, subset=\"training\")\n",
    "val_ds = create_image_dataset(data_dir=\"../../scratch/train/\", batch_size=32, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "682bfd1d-9d69-4fa6-bdb1-fe0b75185c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/model_builder.py\n",
    "\n",
    "#\n",
    "# a function that defines, builds and compiles a model\n",
    "#\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "inputShape=(96, 96, 1)\n",
    "data_format=\"channels_last\"\n",
    "\n",
    "def model_builder(hp, input_shape):\n",
    "    model = keras.Sequential(name=\"fingerprint_prediction\")\n",
    "    \"\"\"\n",
    "    Builds and compiles a simple sequential model.\n",
    "    :param hp: The optimal parameters for training.\n",
    "    :param input_shape: The shape of the input data.\n",
    "    :return: The compiled model.\n",
    "    \"\"\"\n",
    "    # comment out to remove augmentation\n",
    "    data_augmentation\n",
    "    input_shape=(img_height, img_width, 1)\n",
    "    chanDim = -1\n",
    "        \n",
    "    # first CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_1\", min_value=32, max_value=96, step=32),\n",
    "        (3, 3), padding=\"same\", input_shape=inputShape, data_format=data_format))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=data_format))\n",
    "    \n",
    "    # second CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_2\", min_value=64, max_value=128, step=32),\n",
    "        (3, 3), padding=\"same\", data_format=data_format))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=data_format))\n",
    "    \n",
    "    # third CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_3\", min_value=96, max_value=256, step=32),\n",
    "        (3, 3), padding=\"same\", data_format=data_format))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=data_format))    \n",
    "    \n",
    "    # first (and only) set of FC => RELU layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int(\"dense_units\", min_value=256,\n",
    "                           max_value=768, step=256)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # softmax classifier\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "        # initialize the learning rate choices and optimizer\n",
    "    lr = hp.Choice(\"learning_rate\",\n",
    "                   values=[1e-1, 1e-2, 1e-3])\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "   \n",
    "    # return the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        # metrics to be evaluated by the model during training and testing.The strings 'accuracy' or 'acc', TF converts this to binary, categorical or sparse.\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3426d0f9-c1ab-499d-8450-132a42a409a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../scratch/src/run_hyperband.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scratch/src/run_hyperband.py\n",
    "\n",
    "from kerastuner import Hyperband\n",
    "import tensorflow as tf\n",
    "\n",
    "communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "    implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)\n",
    "\n",
    "def run_hyperband(model_builder, x_train, y_train, x_val, y_val, project_name):\n",
    "    # Initialize the Hyperband tuner\n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=5,\n",
    "        factor=3,\n",
    "        project_name='hypertune',\n",
    "        distribution_strategy=strategy,\n",
    "        directory='../../scratch/tune/model_hp',\n",
    "        overwrite=True\n",
    "\n",
    "    )\n",
    "\n",
    "    # Fit the tuner to the training data\n",
    "    tuner.search(x_train, y_train, epochs=max_epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "    # Get the best model from the search\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Evaluate the best model on the validation data\n",
    "    _, acc = best_model.evaluate(x_val, y_val, verbose=0)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb64e45-3d6d-40e1-9be4-c73a71ebf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile src/train_model.py\n",
    "\n",
    "def train_model(model, X_train, y_train, batch_size=32, epochs=10, validation_split=0.2):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087db9ca-c5f4-44c0-bc68-4680a6c1c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_model(model, dataset, epochs, communication_options):\n",
    "    \"\"\"\n",
    "    Trains a model using data parallelism with the specified communication options.\n",
    "    :param model: The model to train.\n",
    "    :param dataset: The dataset to use for training.\n",
    "    :param epochs: The number of epochs to train the model for.\n",
    "    :param communication_options: The communication options to use when training.\n",
    "    \"\"\"\n",
    "    # Use the MirroredStrategy for data parallelism\n",
    "    communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "            # AUTO defers the choice to Tensorflow.\n",
    "            implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\n",
    "    \n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Use the experimental CommunicationOptions to specify communication options\n",
    "        dataset = dataset.batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE,\n",
    "                                                        communication_options=communication_options)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(dataset, epochs=epochs)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66a55d-d714-4a13-855a-fcf63a52a194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
