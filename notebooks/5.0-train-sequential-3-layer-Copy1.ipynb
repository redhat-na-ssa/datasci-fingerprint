{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb86f8e1-b38b-4d0c-92b5-257c657ce304",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0685747-4565-4cf5-ae05-ed5d7ea051da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 04:37:57.178255: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 04:37:57.435204: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-24 04:38:00.479129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-24 04:38:00.479229: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-24 04:38:00.479240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.1\n"
     ]
    }
   ],
   "source": [
    "# Install packages and frameworks\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# expecting 2.11\n",
    "# if 2.7, than logging errors will show \"Cleanup called...\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba71e1f6-f29c-4c35-8a92-a60c8851fd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SCRATCH=../scratch\n"
     ]
    }
   ],
   "source": [
    "# scratch directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "! [ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', './scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfc60d-2aec-4516-8cb7-8837441b8391",
   "metadata": {},
   "source": [
    "# Load the saved datasets\n",
    "\n",
    "The TFRecord format is a simple format for storing a sequence of binary records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165835a8-5f4b-4c1d-9dfa-461e290b9b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 04:38:03.623398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:03.626605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:03.753688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:03.756473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:03.759379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:03.762180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:03.765514: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 04:38:04.160864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:04.162725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:04.164491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:04.166162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:04.167795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:04.169369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:06.209721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:06.211681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:06.213539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:06.215172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:06.216808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:06.218400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13595 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1d.0, compute capability: 7.5\n",
      "2023-05-24 04:38:06.218832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-24 04:38:06.220462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13595 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 96, 96, 1), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = scratch_path + '/tf_datasets/train/'\n",
    "train_ds = tf.data.Dataset.load(path)\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368c217f-f53f-4700-8fdc-19a14528a37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 96, 96, 1), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = scratch_path + '/tf_datasets/validate/'\n",
    "validation_ds = tf.data.Dataset.load(path)\n",
    "validation_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f42449-1ee8-4a92-a2af-a8d484843a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 96, 96, 1), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = scratch_path + '/tf_datasets/test/'\n",
    "test_ds = tf.data.Dataset.load(path)\n",
    "test_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b957a6-aa33-4145-b9b2-21f5eb039539",
   "metadata": {},
   "source": [
    "## Configure the datasets for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "1. `Caching` a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.\n",
    "1. `Prefetching` overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027e6f00-e003-414a-a74e-122c67cc0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da87c9-0c6b-49af-ae0e-c0f3b98515a0",
   "metadata": {},
   "source": [
    "# Check the device spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c352932-39d4-4ea2-adc8-c761506bf7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display physical devices\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e7ec196-e0b8-493c-9856-56da8d08ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ['/device:GPU:0', '/device:GPU:1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a3172-4779-4d87-a645-8f28bff0f12f",
   "metadata": {},
   "source": [
    "# Set training strategy\n",
    "\n",
    "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "\n",
    "```\n",
    "#  example, single device\n",
    "#device=\"/CPU:0\"\n",
    "#device=\"/GPU:0\"\n",
    "\n",
    "#  example, multiple devices devices\n",
    "#device = ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3']\n",
    "```\n",
    "\n",
    "One thing to know: when keras run with gpu, it uses almost all vram. So we needed to give memory_limit for each notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4642e0f-b5b0-447f-998d-19cd52eaaa8d",
   "metadata": {},
   "source": [
    "### One Device Strategy\n",
    "\n",
    "A distribution strategy for running on a single device. Device string identifier for the device on which the variables should be placed. See class docs for more details on how the device is used. Examples: \"/cpu:0\", \"/gpu:0\", \"/device:CPU:0\", \"/device:GPU:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bec8bef8-6c4c-44c8-b6d3-cce296ca8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategy = tf.distribute.OneDeviceStrategy(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e751b6-8f3b-4278-8820-e4129a73617d",
   "metadata": {},
   "source": [
    "### Mirrored Strategy \n",
    "\n",
    "Supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b74874-3bfc-4c2f-b61d-1b97bb82e36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908cc07-24ae-4fda-9c62-ef74d3fab75d",
   "metadata": {},
   "source": [
    "### Multi-Worker Mirrored Strategy\n",
    "\n",
    "Multi-Worker MirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it creates copies of all variables in the model on each device across all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69fb1736-3ffc-44d1-819b-a98413c28bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to create a Multi-worker Mirrored Strategy\n",
    "\n",
    "#communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "    # RING is RPC-based and supports both CPUs and GPUs.\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.RING)\n",
    "\n",
    "    # NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\n",
    "    \n",
    "    # AUTO defers the choice to Tensorflow.\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\n",
    "\n",
    "#strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d3709-db26-4a83-ac98-a36013a86e76",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443bfbbf-3f3f-45a2-9586-85bf8d04657d",
   "metadata": {},
   "source": [
    "## Set some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d699ebcd-4965-416a-93a6-654e49fd0d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables for consistency\n",
    "img_height = 96              # desired height\n",
    "img_width = 96               # desired width\n",
    "batch_size = 32              # batch inputs in 32\n",
    "seed_train_validation = 42   # Must be same for train_ds and val_ds\n",
    "validation_split = 0.3       # move 30% of the data into validation\n",
    "class_names = ['left', 'right']\n",
    "\n",
    "dataFormat=\"channels_last\"\n",
    "num_classes = len(class_names)\n",
    "inputShape=(img_height, img_width, 1)\n",
    "chanDim = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32f43e-eb02-4411-b593-99e860cdd4e0",
   "metadata": {},
   "source": [
    "tf.keras models are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:\n",
    "\n",
    "## Setup the layers\n",
    "\n",
    "The Sequential model consists of three convolution blocks (tf.keras.layers.Conv2D) with a max pooling layer (tf.keras.layers.MaxPooling2D) in each of them. There's a fully-connected layer (tf.keras.layers.Dense) with 128 units on top of it that is activated by a ReLU activation function ('relu'). This model has not been tuned in any way—the goal is to show you the mechanics using the datasets you just created. To learn more about image classification, visit the Image classification tutorial.\n",
    "\n",
    "Here's an example of Python code using Keras to create a sequential classification model that accepts 96x96x1 input images.\n",
    "\n",
    "In this example, we start with three convolutional layers, each followed by a max pooling layer to downsample the input. Then, we flatten the output from the convolutional layers and add two fully connected layers. Finally, we add an output layer with the number of classes and compile the model using an optimizer (e.g., Adam) and a loss function (e.g., categorical cross-entropy).\n",
    "\n",
    "Make sure to replace num_classes with the actual number of classes in your classification problem.\n",
    "```\n",
    "import tensorflow as tf    \n",
    "\n",
    "model = tf.keras.Model(...)\n",
    "\n",
    "# Run training on GPU\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(...)\n",
    "\n",
    "# Run inference on CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    model.predict(...)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122651a9-8379-408c-9bae-1d018f0b496d",
   "metadata": {},
   "source": [
    "# Set up the layers\n",
    "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
    "\n",
    "Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6f6b0d3-6ade-4496-b898-4553258b2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=inputShape),\n",
    "        tf.keras.layers.Reshape(target_shape=inputShape),\n",
    "        tf.keras.layers.Conv2D(96, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(96, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(96, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19a77c-2cee-49e4-9da7-3841f2b320ec",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    "Loss function —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "Optimizer —This is how the model is updated based on the data it sees and its loss function.\n",
    "Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0547dca-a170-49e6-a1fe-198a5da84176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 96)        960       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 47, 47, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 45, 45, 96)        83040     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 22, 22, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 96)        83040     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 10, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2457856   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,625,410\n",
      "Trainable params: 2,625,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "                  from_logits=False,\n",
    "                  label_smoothing=0.0,\n",
    "                  axis=-1,\n",
    "                  #reduction=losses_utils.ReductionV2.AUTO,\n",
    "                  name='categorical_crossentropy'),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ee8cf-58e1-46aa-95ba-82d89736d9ea",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Training the neural network model requires the following steps:\n",
    "\n",
    "Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays.\n",
    "The model learns to associate images and labels.\n",
    "You ask the model to make predictions about a test set—in this example, the test_images array.\n",
    "Verify that the predictions match the labels from the test_labels array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f24a68a-4ac5-47fe-a429-4852569d80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 04:52:14.706369: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"LoadDataset/_1\"\n",
      "op: \"LoadDataset\"\n",
      "input: \"Const/_0\"\n",
      "attr {\n",
      "  key: \"Treader_func_args\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 393\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"compression\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 2\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"reader_func\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_load_lambda_14\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 04:52:22.277034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8700\n",
      "2023-05-24 04:52:22.381769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8700\n",
      "2023-05-24 04:52:27.808848: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fe8dcd5c300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-24 04:52:27.808892: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-05-24 04:52:27.808900: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "2023-05-24 04:52:27.816473: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-24 04:52:27.952208: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393/393 [==============================] - ETA: 0s - loss: 1.7880 - accuracy: 0.6428"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 04:52:35.128962: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"LoadDataset/_1\"\n",
      "op: \"LoadDataset\"\n",
      "input: \"Const/_0\"\n",
      "attr {\n",
      "  key: \"Treader_func_args\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 169\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"compression\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 2\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"reader_func\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_load_lambda_30\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393/393 [==============================] - 23s 23ms/step - loss: 1.7880 - accuracy: 0.6428 - val_loss: 0.5870 - val_accuracy: 0.7098\n",
      "Epoch 2/10\n",
      "393/393 [==============================] - 7s 19ms/step - loss: 0.6350 - accuracy: 0.6201 - val_loss: 0.6921 - val_accuracy: 0.5035\n",
      "Epoch 3/10\n",
      "393/393 [==============================] - 7s 19ms/step - loss: 0.6837 - accuracy: 0.5461 - val_loss: 0.6760 - val_accuracy: 0.5485\n",
      "Epoch 4/10\n",
      "393/393 [==============================] - 7s 19ms/step - loss: 0.6817 - accuracy: 0.5470 - val_loss: 0.6879 - val_accuracy: 0.5234\n",
      "Epoch 5/10\n",
      "393/393 [==============================] - 7s 19ms/step - loss: 0.6842 - accuracy: 0.5442 - val_loss: 0.6816 - val_accuracy: 0.5551\n",
      "Epoch 6/10\n",
      "393/393 [==============================] - 7s 18ms/step - loss: 0.6858 - accuracy: 0.5419 - val_loss: 0.6899 - val_accuracy: 0.5059\n",
      "Epoch 7/10\n",
      "393/393 [==============================] - 7s 18ms/step - loss: 0.6887 - accuracy: 0.5252 - val_loss: 0.6798 - val_accuracy: 0.5230\n",
      "Epoch 8/10\n",
      "393/393 [==============================] - 7s 19ms/step - loss: 0.6855 - accuracy: 0.5253 - val_loss: 0.6778 - val_accuracy: 0.5620\n",
      "Epoch 9/10\n",
      "393/393 [==============================] - 7s 19ms/step - loss: 0.6809 - accuracy: 0.5415 - val_loss: 0.6567 - val_accuracy: 0.5965\n",
      "Epoch 10/10\n",
      "393/393 [==============================] - 7s 19ms/step - loss: 0.6805 - accuracy: 0.5432 - val_loss: 0.6727 - val_accuracy: 0.5395\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model.fit(train_ds, epochs=10, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6b0f0-9baf-4611-be69-3d63c41cb83c",
   "metadata": {},
   "source": [
    "# Evaluate accuracy\n",
    "\n",
    "Next, compare how the model performs on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f34ff61-92d4-4f93-9df6-a9750100b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 04:53:53.159648: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"LoadDataset/_1\"\n",
      "op: \"LoadDataset\"\n",
      "input: \"Const/_0\"\n",
      "attr {\n",
      "  key: \"Treader_func_args\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 169\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"compression\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 2\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"reader_func\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_load_lambda_30\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 - 1s - loss: 0.6727 - accuracy: 0.5395 - 1s/epoch - 8ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46843f6-1787-4a36-9d08-fbbec3e39d0c",
   "metadata": {},
   "source": [
    "# Print Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24fdc063-7afd-4b79-bfdf-6f9ec5480104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.5395469665527344\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1ab9f-9abc-4770-993e-ade624368f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
