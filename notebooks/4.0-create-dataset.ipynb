{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc8c750b-1325-4adf-88e8-88a35b023ad0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e16e19-702e-46e4-824b-e99185dd4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and frameworks\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# expecting 2.11\n",
    "# if 2.7, than logging errors will show \"Cleanup called...\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f3f6e-6924-45ad-ba89-8aa21dc985e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "! [ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', './scratch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64e973ed-5482-47f3-8885-d2efacefb33b",
   "metadata": {},
   "source": [
    "## Cleanup original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01522e96-54fa-402c-a7b8-08424e67c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "path = scratch_path + '/train'\n",
    "\n",
    "if os.path.exists(path) and os.path.isdir(path):\n",
    "    # Directory exists, execute your code here\n",
    "    print(\"Directory exists. Removing...\")\n",
    "    shutil.rmtree(path)\n",
    "    # Your code goes here\n",
    "else:\n",
    "    print(\"Directory does not exist.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72191ab9-2c62-4b1b-8ee0-25e6d2aed816",
   "metadata": {},
   "source": [
    "# Split the data into Train, Validation and Test\n",
    "\n",
    "Keras utility generates a dataset in tf.data.Dataset format from image files in a directory and infers the labels based on the parent folder. This utility will return a tf.data.Dataset that yields batches of images from the subdirectories left and right\n",
    "\n",
    "```\n",
    "train_lr/\n",
    "├── left/\n",
    "│   ├── a_image_1.jpg\n",
    "│   └── a_image_2.jpg\n",
    "└── right/\n",
    "    ├── b_image_1.jpg\n",
    "    └── b_image_2.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724be93-afba-4144-b1b6-87b71e3db046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables for consistency\n",
    "img_height = 96              # desired height\n",
    "img_width = 96               # desired width\n",
    "batch_size = 32              # batch inputs in 32\n",
    "seed_train_validation = 42   # Must be same for train_ds and val_ds\n",
    "validation_split = 0.3       # move 30% of the data into validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "664e0435-33b3-4cd0-b360-264e8be40ebb",
   "metadata": {},
   "source": [
    "## Create some new directories to save our prepared datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0c741-a0ce-480f-9a88-d80962d4f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p \"${SCRATCH}\"/tf_datasets/{train,validate,test}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e044a4-86cb-455f-8bd2-0b3e657a10f9",
   "metadata": {},
   "source": [
    "## Create Train\n",
    "\n",
    "Train is the sample of data used to fit the model. Let's generate a tf.data.Dataset from the processed training examples and infer the labels from the directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffe794-a4e4-41c2-8b28-bb6f995b4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order for keras to infer the labels, you cannot have any \"extra\" subdirectories that do not match your expected labels\n",
    "\n",
    "!rm -rf scratch_path + '/train_lr/.ipynb_checkpoints'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bc09534-6c80-4186-ab72-8afeffc70b31",
   "metadata": {},
   "source": [
    "The saved dataset is saved in multiple file \"shards\". By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the shard_func function. For example, you can save the dataset to using a single shard as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d29e4de3-1467-4899-a404-162a1f025034",
   "metadata": {},
   "source": [
    "def custom_shard_func(element):\n",
    "    return np.int64(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dec14-fa70-40bd-9826-136705b63c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    scratch_path + '/train_lr',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# save the dataset\n",
    "dir = scratch_path + \"/tf_datasets/train\"\n",
    "tf.data.Dataset.save(train_ds,\n",
    "                     dir, \n",
    "                     #compression=None,\n",
    "                     #shard_func=custom_shard_func,\n",
    "                     checkpoint_args=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4f60f83-8517-4739-8c81-aea2a1bc19ce",
   "metadata": {},
   "source": [
    "## Create Validation\n",
    "\n",
    "Validation is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eead56f-71bd-4062-a334-f3b4cb17ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the validation dataset\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    scratch_path + '/train_lr',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# save the dataset\n",
    "dir = scratch_path + \"/tf_datasets/validate\"\n",
    "tf.data.Dataset.save(validation_ds,\n",
    "                     dir, \n",
    "                     #compression=None,\n",
    "                     #shard_func=custom_shard_func,\n",
    "                     checkpoint_args=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0fd887e-0048-4e1e-a036-7788571a541f",
   "metadata": {},
   "source": [
    "## Create Test\n",
    "\n",
    "The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215771a6-fb81-499f-805a-3387163fb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "test_ds = validation_ds.take(16)\n",
    "validation_ds = validation_ds.skip(16)\n",
    "\n",
    "# save the datasets\n",
    "dir = scratch_path + \"/tf_datasets/test\"\n",
    "tf.data.Dataset.save(test_ds,\n",
    "                     dir, \n",
    "                     #compression=None,\n",
    "                     #shard_func=custom_shard_func,\n",
    "                     checkpoint_args=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93ee5113-9c59-47cf-b151-2ba0a6067d91",
   "metadata": {},
   "source": [
    "You now have a train, validation, and test dataset written to a directory. tf.data.Dataset.save() is used to save the dataset to the specified save_dir. Make sure to provide a valid path to the directory where you want to save the dataset. The dataset will be saved in a sharded file format.\n",
    "\n",
    "Later, if you want to load the saved dataset, you can use tf.data.Dataset.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d37aaf71-aeeb-4ce6-8bc9-d63b9eca2690",
   "metadata": {},
   "source": [
    "## Print the Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ab2f2-b822-4e99-96c9-2eb0808d5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserves 393 batches training\n",
    "print('70% for training -->', train_ds.cardinality())\n",
    "# reserves 164 batches validation\n",
    "print('20% for validating -->', validation_ds.cardinality())\n",
    "# reserves 5 batches testing\n",
    "print('10% for testing -->', test_ds.cardinality())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d177737-e37e-4653-827c-2c5fd0324fb9",
   "metadata": {},
   "source": [
    "## Print Inferred Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ac758-4c0a-4b5b-9e20-8bbab5bcb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the class names inferred from the training dataset\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
