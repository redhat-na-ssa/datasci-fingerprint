{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bf1b9c-0994-41ea-be22-c887d5c6aedd",
   "metadata": {},
   "source": [
    "# Fingerprint Left or Right Hand Prediction\n",
    "\n",
    "## About the data\n",
    "Sokoto Coventry Fingerprint Dataset (SOCOFing) is a biometric fingerprint database designed for academic research purposes. SOCOFing is made up of 6,000 fingerprint images from 600 African subjects and contains unique attributes such as labels for gender, hand and finger name as well as synthetically altered versions with three different levels of alteration for obliteration, central rotation, and z-cut. For a complete formal description and usage policy please refer to the following paper: https://arxiv.org/abs/1807.10609.\n",
    "\n",
    "## About the notebook\n",
    "The intention of this notebook is to demonstrate steps from data ingestion to model saving that provides an accurate enough model that predicts if a fingerprint comes from a left or right hand. Coupled with other models that accurately predict finger and gender is valuable when matching against other identifiable information.\n",
    "\n",
    "1. *Data Ingestion* [from object storage](#working-with-s3-buckets)\n",
    "1. *Dataset preparation* (infer labels, splitting, augmenting, optimizing)\n",
    "1. *Model Development* from scratch and *Training Strategies* (one device, mirrored, multi-worker mirrored)\n",
    "1. *Model Performance* Hyperparameter Tuning strategies (RandomSearch, Hyperband, BayesianOptimization, Sklearn)\n",
    "1. *Model Serialization* to object storage\n",
    "1. *Prediction Sampling*\n",
    "\n",
    "### Notebook Tested Requirements\n",
    "\n",
    "|Notebook origin|Notebook Customization|Instance Type|Kernel|TensorFlow|Runtime|\n",
    "|:-------|:-------|:-------|:-------|:-------|:-------|\n",
    "|SageMaker Notebook Instances|[from GitHub](https://github.com/redhat-na-ssa/demo-rosa-sagemaker/blob/main/sagemaker/lifecycle-from-github.sh)|ml.m5.4xlarge (vCPU: 16, RAM: 64 GiB)|conda_tensorflow2_p310|2.11|~120 minutes|\n",
    "|SageMaker Notebook Instances|[from GitHub](https://github.com/redhat-na-ssa/demo-rosa-sagemaker/blob/main/sagemaker/lifecycle-from-github.sh)|ml.p3.8xlarge (vCPU: 32, RAM: 244 GiB)|conda_tensorflow2_p310|2.11|~15 minutes|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa098c-5402-4108-9a5d-4ea778fab9d7",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "If this is your first time running the notebook, you may need to restart the kernel after the Tensorflow upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bdb2c3-198b-4c84-ac65-988a9a735749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can run individual functions!\n",
      "\n",
      "example:\n",
      "  setup_demo\n",
      "\n",
      "Pulling dataset from https://github.com/redhat-na-ssa/datasci-fingerprint-data.git...\n",
      "exists\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.fflags'\n"
     ]
    }
   ],
   "source": [
    "# source the setup Bash script to run specific configuration tasks\n",
    "! source ../setup.sh && setup_dataset && install_requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d191b324-1a31-4b3a-a481-349730db73e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 15:41:17.333121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 15:41:17.594857: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-23 15:41:20.574571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-23 15:41:20.574656: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-23 15:41:20.574667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.1\n"
     ]
    }
   ],
   "source": [
    "# Install packages and frameworks\n",
    "\n",
    "# uncomment below if using a notebook with a sagemaker notebook instance lifecycle config\n",
    "#! pip install -U pip --quiet\n",
    "#! pip install -r ../requirements.txt --quiet\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# debugging code \"Cleanup Called...\" gets displayed if get_logger is not set\n",
    "# the below code suppresses the \"Cleanup Called...\" output\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# expecting 2.11\n",
    "# if 2.7, than logging errors will show \"Cleanup called...\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edad1efc-dd91-43ce-b0a4-a90ef8eacdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SCRATCH=../scratch\n"
     ]
    }
   ],
   "source": [
    "# scratch directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "! [ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', './scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad3d2f3-404e-4a87-91e9-177b7219cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p \"${SCRATCH}\"/{real,tune,train,tf_datasets,train_lr/{left,right}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ac1ab-8500-479c-9698-749f30386ebb",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1facb-2360-4d42-9b1a-ab49d72b215d",
   "metadata": {},
   "source": [
    "## Decompress the data for training\n",
    "\n",
    "Let's check for an existing S3 Bucket for training data.\n",
    "If it's not in S3, we will try other options..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d812792-42f8-4d22-adfc-4aca1e0d9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing s3 bucket\n",
    "! echo S3_BUCKET_DATA=$(aws s3 ls 2>/dev/null | cut -c21- | grep sagemaker-fingerprint-data) > .env\n",
    "\n",
    "# kludge: loadenv from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# if exists, download the objects from s3\n",
    "! [ ! -z \"$S3_BUCKET_DATA\" ] && \\\n",
    "  aws s3 sync s3://${S3_BUCKET_DATA}/train/left $SCRATCH/train/left --quiet && \\\n",
    "  aws s3 sync s3://${S3_BUCKET_DATA}/train/right $SCRATCH/train/right --quiet && \\\n",
    "  aws s3 sync s3://${S3_BUCKET_DATA}/real $SCRATCH/real --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057ff397-1bef-4afe-8d69-615bf4509ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '../scratch/.raw' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# kludge: download dataset from git\n",
    "! git clone https://github.com/redhat-na-ssa/demo-datasci-fingerprint-data.git ${SCRATCH}/.raw\n",
    "\n",
    "! [ ! -d \"${SCRATCH}\"/train/left ] && \\\n",
    "  tar -Jxf ${SCRATCH}/.raw/left.tar.xz -C \"${SCRATCH}\"/train/ && \\\n",
    "  tar -Jxf ${SCRATCH}/.raw/right.tar.xz -C \"${SCRATCH}\"/train/ && \\\n",
    "  tar -Jxf ${SCRATCH}/.raw/real.tar.xz -C \"${SCRATCH}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
