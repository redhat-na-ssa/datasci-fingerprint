{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8c750b-1325-4adf-88e8-88a35b023ad0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41e16e19-702e-46e4-824b-e99185dd4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.1\n"
     ]
    }
   ],
   "source": [
    "# Install packages and frameworks\n",
    "\n",
    "# uncomment below if using a notebook with a sagemaker notebook instance lifecycle config\n",
    "#! pip install -U pip --quiet\n",
    "#! pip install -r ../requirements.txt --quiet\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# debugging code \"Cleanup Called...\" gets displayed if get_logger is not set\n",
    "# the below code suppresses the \"Cleanup Called...\" output\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# expecting 2.11\n",
    "# if 2.7, than logging errors will show \"Cleanup called...\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "507f3f6e-6924-45ad-ba89-8aa21dc985e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SCRATCH=../scratch\n"
     ]
    }
   ],
   "source": [
    "# scratch directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "! [ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', './scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e973ed-5482-47f3-8885-d2efacefb33b",
   "metadata": {},
   "source": [
    "## Cleanup original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01522e96-54fa-402c-a7b8-08424e67c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists. Removing...\n"
     ]
    }
   ],
   "source": [
    "# path\n",
    "path = scratch_path + '/train'\n",
    "\n",
    "if os.path.exists(path) and os.path.isdir(path):\n",
    "    # Directory exists, execute your code here\n",
    "    print(\"Directory exists. Removing...\")\n",
    "    shutil.rmtree(path)\n",
    "    # Your code goes here\n",
    "else:\n",
    "    print(\"Directory does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72191ab9-2c62-4b1b-8ee0-25e6d2aed816",
   "metadata": {},
   "source": [
    "# Split the data into Train, Validation and Test\n",
    "\n",
    "Keras utility generates a dataset in tf.data.Dataset format from image files in a directory and infers the labels based on the parent folder. This utility will return a tf.data.Dataset that yields batches of images from the subdirectories left and right\n",
    "\n",
    "```\n",
    "train_lr/\n",
    "├── left/\n",
    "│   ├── a_image_1.jpg\n",
    "│   └── a_image_2.jpg\n",
    "└── right/\n",
    "    ├── b_image_1.jpg\n",
    "    └── b_image_2.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2724be93-afba-4144-b1b6-87b71e3db046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables for consistency\n",
    "img_height = 96              # desired height\n",
    "img_width = 96               # desired width\n",
    "batch_size = 32              # batch inputs in 32\n",
    "seed_train_validation = 42   # Must be same for train_ds and val_ds\n",
    "validation_split = 0.3       # move 30% of the data into validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e0435-33b3-4cd0-b360-264e8be40ebb",
   "metadata": {},
   "source": [
    "## Create some new directories to save our prepared datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75a0c741-a0ce-480f-9a88-d80962d4f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p \"${SCRATCH}\"/tf_datasets/{train,validate,test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e044a4-86cb-455f-8bd2-0b3e657a10f9",
   "metadata": {},
   "source": [
    "## Create Train\n",
    "\n",
    "Train is the sample of data used to fit the model. Let's generate a tf.data.Dataset from the processed training examples and infer the labels from the directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36ffe794-a4e4-41c2-8b28-bb6f995b4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order for keras to infer the labels, you cannot have any \"extra\" subdirectories that do not match your expected labels\n",
    "\n",
    "!rm -rf scratch_path + '/train_lr/.ipynb_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce6dec14-fa70-40bd-9826-136705b63c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17955 files belonging to 2 classes.\n",
      "Using 12569 files for training.\n"
     ]
    }
   ],
   "source": [
    "# create the training dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    scratch_path + '/train_lr',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# save the dataset\n",
    "dir = scratch_path + \"/tf_datasets/train\"\n",
    "tf.data.Dataset.save(train_ds, dir, compression=None, shard_func=None, checkpoint_args=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f60f83-8517-4739-8c81-aea2a1bc19ce",
   "metadata": {},
   "source": [
    "## Create Validation\n",
    "\n",
    "Validation is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0eead56f-71bd-4062-a334-f3b4cb17ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17955 files belonging to 2 classes.\n",
      "Using 5386 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# create the validation dataset\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    scratch_path + '/train_lr',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# save the dataset\n",
    "dir = scratch_path + \"/tf_datasets/validate\"\n",
    "tf.data.Dataset.save(validation_ds, dir, compression=None, shard_func=None, checkpoint_args=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd887e-0048-4e1e-a036-7788571a541f",
   "metadata": {},
   "source": [
    "## Create Test\n",
    "\n",
    "The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "215771a6-fb81-499f-805a-3387163fb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "test_ds = validation_ds.take(16)\n",
    "validation_ds = validation_ds.skip(16)\n",
    "\n",
    "# save the datasets\n",
    "dir = scratch_path + \"/tf_datasets/test\"\n",
    "tf.data.Dataset.save(test_ds, dir, compression=None, shard_func=None, checkpoint_args=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee5113-9c59-47cf-b151-2ba0a6067d91",
   "metadata": {},
   "source": [
    "You now have a train, validation, and test dataset written to a directory. tf.data.Dataset.save() is used to save the dataset to the specified save_dir. Make sure to provide a valid path to the directory where you want to save the dataset. The dataset will be saved in a sharded file format.\n",
    "\n",
    "Later, if you want to load the saved dataset, you can use tf.data.Dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37aaf71-aeeb-4ce6-8bc9-d63b9eca2690",
   "metadata": {},
   "source": [
    "## Print the Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe1ab2f2-b822-4e99-96c9-2eb0808d5f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70% for training --> tf.Tensor(393, shape=(), dtype=int64)\n",
      "20% for validating --> tf.Tensor(153, shape=(), dtype=int64)\n",
      "10% for testing --> tf.Tensor(16, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# reserves 393 batches training\n",
    "print('70% for training -->', train_ds.cardinality())\n",
    "# reserves 164 batches validation\n",
    "print('20% for validating -->', validation_ds.cardinality())\n",
    "# reserves 5 batches testing\n",
    "print('10% for testing -->', test_ds.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d177737-e37e-4653-827c-2c5fd0324fb9",
   "metadata": {},
   "source": [
    "## Print Inferred Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a5ac758-4c0a-4b5b-9e20-8bbab5bcb091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left', 'right']\n"
     ]
    }
   ],
   "source": [
    "# display the class names inferred from the training dataset\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169603c-766e-4089-9e00-4d2b38a5a1c8",
   "metadata": {},
   "source": [
    "# Apply augmentation\n",
    "When you don't have a large image dataset or when your images are all set in a single direction like ours are, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce over-fitting.\n",
    "\n",
    "Learn more https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9d15064-9c01-48ed-a4fd-41842cceeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "\n",
    "  # randomly rotates images during training\n",
    "  tf.keras.layers.RandomRotation(\n",
    "    # a float represented as fraction of 2 Pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. \n",
    "    0.2,                     # A positive values means rotating counter clock-wise, while a negative value means clock-wise. \n",
    "    fill_mode='constant',    # Points outside the boundaries of the input are filled according to the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n",
    "    interpolation='nearest', # Supported values: \"nearest\", \"bilinear\".\n",
    "    seed=None,               # Integer. Used to create a random seed.\n",
    "    fill_value=0.0           # the value to be filled outside the boundaries when fill_mode=\"constant\".\n",
    "),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8b992-0e57-482e-8ebf-0127e65b2a47",
   "metadata": {},
   "source": [
    "for image, _ in train_ds.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[2]\n",
    "  for i in range(10):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
