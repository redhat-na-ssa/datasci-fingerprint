{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb24089-a3c3-41c6-987c-1474860125de",
   "metadata": {},
   "source": [
    "# Part III: Model Hyperparameter Tuning for Left Right Fingerprints\n",
    "\n",
    "Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n",
    "\n",
    "1. Model hyperparameters which influence model selection such as the number and width of hidden layers\n",
    "1. Algorithm hyperparameters which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier\n",
    "\n",
    "Here are some strategies you can consider while hyperparameter tuning to improve performance:\n",
    "\n",
    "1. Model Complexity (<< we are going to do this one)\n",
    "2. Hyperparameter Tuning (<< and this one)\n",
    "3. Data Augmentation\n",
    "4. Transfer Learning\n",
    "5. Ensemble Methods\n",
    "6. Advanced Techniques\n",
    "\n",
    "The following code will use Keras Hyperband tuning algorithm for adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. [Learn more about tuning here](https://www.tensorflow.org/tutorials/keras/keras_tuner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8881c3-8072-41be-9443-a53abfd73ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1177d5c-c773-4d67-ab13-51f9f9ad1bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Set scratch directory\n",
    "%env SCRATCH=../scratch\n",
    "! [ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', './scratch')\n",
    "\n",
    "# Define image dataset creation function\n",
    "def create_image_dataset(directory, color_mode=\"grayscale\", validation_split=0.2,\n",
    "                         subset=None, seed=123, image_size=(96, 96), batch_size=32):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        color_mode=color_mode,\n",
    "        validation_split=validation_split,\n",
    "        subset=subset,\n",
    "        seed=seed,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    dataset = dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_ds = create_image_dataset(\n",
    "    scratch_path + '/processed/hand',\n",
    "    subset=\"training\",\n",
    ")\n",
    "val_ds = create_image_dataset(\n",
    "    scratch_path + '/processed/hand',\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "# Define model creation function for hyperparameter tuning\n",
    "def create_hp_model(hp):\n",
    "    class_names = ['left', 'right']  # Define default class names here\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Define hyperparameters to tune\n",
    "    hp_filters_1 = hp.Int('filters_1', min_value=16, max_value=64, step=16)\n",
    "    hp_filters_2 = hp.Int('filters_3', min_value=64, max_value=256, step=64)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=64, max_value=256, step=64)\n",
    "    \n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(hp_filters_1, 3, padding='same', activation='relu', input_shape=(96, 96, 1)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(hp_filters_2, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(hp_filters_2, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(hp_filters_2, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(hp_dense_units, activation='relu'),\n",
    "        layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Perform hyperparameter tuning with Hyperband tuner\n",
    "tuner = kt.Hyperband(\n",
    "    create_hp_model,\n",
    "    max_epochs=10,\n",
    "    objective='val_accuracy',\n",
    "    factor=3,\n",
    "    directory=scratch_path + '/models',\n",
    "    project_name='model_tuning'\n",
    ")\n",
    "\n",
    "# Create a callback to stop training early after reaching a certain value for the validation loss.\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Define a callback to print best value so far\n",
    "class PrintBestValue(tf.keras.callbacks.Callback):\n",
    "    def on_trial_end(self, trial, logs=None):\n",
    "        print(f\"Best val_accuracy So Far: {self.model.oracle.get_best_trials(1)[0].score}\")\n",
    "        \n",
    "tuner.search(train_ds, validation_data=val_ds, epochs=10, batch_size=32, callbacks=[PrintBestValue()])\n",
    "\n",
    "# Get best hyperparameters and build the model\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# Train the best model with the obtained best hyperparameters\n",
    "history = best_model.fit(train_ds, validation_data=val_ds, epochs=10, batch_size=32, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d7b0a-99d7-4882-bf9c-ebb82ff934b4",
   "metadata": {},
   "source": [
    "Overall, a low training loss and high accuracy along with reasonably close validation metrics indicate that the model is learning well and has a decent ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab703da-f1f2-4ffd-895d-8dd2db41cfbe",
   "metadata": {},
   "source": [
    "## Performance Visualization Function\n",
    "\n",
    "This function, plot_training_history, takes in the history object returned by the model's fit method and the epochs parameter. It then plots the training and validation accuracy as well as the training and validation loss in two subplots, providing insights into the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727f9d1-758f-4444-b30c-74a3a4bc3305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_training_history(history, epochs, save_dir=None):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    # Save the plot to a directory if specified\n",
    "    if save_dir:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        plot_filename = f\"tuning_history_plot_{current_time}.png\"\n",
    "        plot_path = os.path.join(save_dir, plot_filename)\n",
    "        \n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Plot saved to: {plot_path}\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dc5f4-0cd0-40a3-b31c-fc58b3a5d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the performance visualization\n",
    "plot_training_history(history=history, epochs=10, save_dir='../reports/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d6a77-6f41-45ca-afd7-43ab04cf8c3d",
   "metadata": {},
   "source": [
    "# Serialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38dba8-2bc4-475e-a496-535fff5c0f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note: tf.saved_model.save(model, path_to_dir)\n",
    "model_version = os.environ.get('VERSION', \"2\")\n",
    "\n",
    "model_path = scratch_path + \"/models/hand/\" + model_version + \"/model.savedmodel\"\n",
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde16544-3bef-4573-8741-991f6dcfeddd",
   "metadata": {},
   "source": [
    "## Load the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84972fb7-fe35-4941-9ff9-03e70fc17808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda69f5-3952-4b54-b09c-2cb595b39363",
   "metadata": {},
   "source": [
    "# Test with a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e88fe-1558-4456-8097-76f4ef74a716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select random images\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples_path = scratch_path + \"/processed/real/\"\n",
    "file_list = os.listdir(samples_path)\n",
    "print('Files in path: ' + str(len(file_list)))\n",
    "\n",
    "test_image = random.choice(file_list)\n",
    "print('Selected: ' + test_image)\n",
    "\n",
    "# loads an image into PIL format.\n",
    "img = tf.keras.utils.load_img(\n",
    "    samples_path + test_image,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(96, 96),\n",
    "    #interpolation='nearest',\n",
    ")\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "# converts a PIL image instance to a numpy array\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "print(\"shape:\",img_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01abc797-629d-4b0f-acd3-abfeed0aac76",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfca2ba-581c-4996-86c7-f8b2d5001ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "class_mapping = {0: 'left', 1: 'right'}  # Define your class mapping\n",
    "\n",
    "predicted_label = class_mapping[np.argmax(score)]\n",
    "confidence = 100 * np.max(score)\n",
    "\n",
    "print(\n",
    "    f\"This image {test_image} is predicted as {predicted_label} with a {confidence:.2f}% confidence.\"\n",
    ")\n",
    "\n",
    "# Check if the predicted label is in the test image name (case insensitive)\n",
    "is_label_in_image_name = predicted_label.lower() in test_image.lower()\n",
    "print(f\"Is predicted label in the image name? {is_label_in_image_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e5ee9-3646-4da6-9289-f477095897fc",
   "metadata": {},
   "source": [
    "# Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301da99c-ee82-40e8-b51a-36cd4f47c565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list objects using the aws s3 cli\n",
    "! aws s3 ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58326f71-0240-4930-9993-83a800300e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List the dynamically created bucket for the demo\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "filtered_buckets=[]\n",
    "\n",
    "for bucket in response['Buckets']:\n",
    "    bucket_name = bucket['Name']\n",
    "    if bucket_name.startswith('sagemaker-fingerprint-'):\n",
    "        filtered_buckets.append(bucket_name)\n",
    "    \n",
    "print(filtered_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f6d58-ae60-4919-a26b-b0815eaa3640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the model to the demo bucket\n",
    "import subprocess\n",
    "\n",
    "for bucket in filtered_buckets:\n",
    "    command = f\"aws s3 sync ../scratch/models/hand s3://{bucket}/models/hand\"\n",
    "    subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fef4a-78c0-410e-9c41-9d8be9e799b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
